	The file estimate.py contains a method which estimates whether a previously unseen extraction is valid or not. The method takes as its input collections of extractions that were marked as correct or incorrect in the gold standard. It also takes the collection of Nemex extractions that were not found in the gold standard, i.e. the 'unknown' extractions whose labels are to be estimated. Estimation is based off tf-idf scoring and vector similarity found in Information Retrieval.
	The file estimate.py contains three methods. The first (estimate_weights) is the 'head' method, which calls the other two methods and returns the results of estimating whether an extraction is correct or not. It is important to note that all calculations made are done on both the correct AND the incorrect extractions found by Nemex in the gold standard.
	The second method (calculate_tfs_idfs) calculates the tf-idf scores of all tokens found in the collections of Nemex's correct and incorrect extractions from the gold standard. In this case, the extractions found for each each sentence from the data set are combined and treated as a 'document'. In the case of the NYT dataset, there would be 200 'documents' for the correct extrations and 200 for the incorrect extractions. The term frequencies are done at the sentence level and the inverse document frequencies are taken over the set of all 'documents', i.e. the sentences.
	The third method (cos_sim) compares unknown extractions against the correct and incorrect extractions for a given sentence. This is done by using the cosine similarity of two vectors. Like in Information Retrieval, the unknown extraction to be labeled is treated as a 'query'. This 'query' retrieves the 'document' from each sentence that it is most similar to. For each sentence, there are only 2 documents. One document represents the correct extractions, the other represents the incorrect ones. The query has to have its own tf-idf score computed on the fly and then be represented as a vector. The correct and incorrect documents for a sentence are also represented as vectors. Then, two similarity scores are computed; the similarity of the 'query' to the correct 'document' and the similarity of the 'query' to the incorrect 'document'. The unknown extraction is then labeled as correct or incorrect according to the higher similarity score.
	In some cases, the 'query' and its tokens will not be present in the correct and/or incorrect 'documents' for a given sentence. If a query's tokens are not present in one document, but are present in the other, then it will be labeled with the same label as the other document. For instance, if a query's tokens are only found in the correct document for a given sentence, then it will be marked as correct. When the query's tokens are not found in either document for a given sentence, this special instance is labeled with '0.5'. This instance occurs very rarely.
	In most cases, the unknown extractions can be labeled. Further examination is required to evaluate the effectiveness of this estimation method, as well as what can be done about the extractions for which no label can be assigned. For this purpose, this script prints out all novel tokens found in extractions after assigning them a label of correct or incorrect. The counts of these tokens are also included.

